{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keqingli1129/langchain/blob/main/building_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLCFhtkodbTT"
      },
      "source": [
        "Agents\n",
        "\n",
        "1. Scratch -> LLM + functions inside its prompt\n",
        "2. OpenAI Function calling -> using the official api to call functions for LLMs\n",
        "3. Build simple agents with langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3WT58ATdbTX"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWL6GWpdbTZ",
        "outputId": "668245d6-a8a1-4dd2-b26a-3f7452ff91e5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "1. Create a new directory: Use the `mkdir` command followed by the directory name to create a new directory in your current location. For example, to create a directory called \"my_folder\", type `mkdir my_folder`.\n",
              "\n",
              "2. Rename a file: Utilize the `mv` command followed by the current file name and the desired new file name to rename a file. For instance, to rename a file named \"old_file.txt\" to \"new_file.txt\", type `mv old_file.txt new_file.txt`.\n",
              "\n",
              "3. List directory contents: Use the `ls` command to display all the files and directories in your current location. Simply type `ls` and press enter to list the contents."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from IPython.display import Markdown\n",
        "client = OpenAI()\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "output = get_response(\"Create a simple task list of 3 desktop things I can do on the terminal.\")\n",
        "Markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDMNuu_ldbTa"
      },
      "source": [
        "Ok cool, so here we have three ideas of actions to perform:\n",
        "\n",
        "- Creating directories\n",
        "- Listing files\n",
        "- Removing files\n",
        "\n",
        "Let's transform them into functions that we could call just like in any type of Python-based application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrnswG-ydbTa"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def create_directory():\n",
        "    subprocess.run([\"mkdir\", \"test\"])\n",
        "\n",
        "def create_file():\n",
        "    subprocess.run([\"touch\", \"test.txt\"])\n",
        "\n",
        "def list_files():\n",
        "    subprocess.run([\"ls\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcHNWy4odbTa",
        "outputId": "3d2d080e-90bf-4f0c-8ab4-7b2f1bb9de93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb   building-agents.ipynb \u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjRefCO1dbTb",
        "outputId": "f0448e60-e547-4cbc-bb7f-d2fd9967e9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb   \u001b[1m\u001b[36mtest\u001b[m\u001b[m\n",
            "building-agents.ipynb \u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "create_directory()\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMtzKQh4dbTb",
        "outputId": "ac5848cc-ffe4-43c0-817a-0dd4c68dc12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb   \u001b[1m\u001b[36mtest\u001b[m\u001b[m                  test.txt\n",
            "building-agents.ipynb \u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "create_file()\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuAIJ8TCdbTb",
        "outputId": "911dca46-8385-4e2d-81e6-5a640e4b276b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb\n",
            "building-agents.ipynb\n",
            "\u001b[1m\u001b[36mtest\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n",
            "test.txt\n"
          ]
        }
      ],
      "source": [
        "list_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7xHtw-7dbTc"
      },
      "source": [
        "['Toolformer'](https://arxiv.org/pdf/2302.04761.pdf) demonstrated!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM-EWNyadbTc"
      },
      "outputs": [],
      "source": [
        "class ModelWithTools:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANR76_HsdbTc"
      },
      "source": [
        "OK, cool! now Notice that, here we added single parameters to the functions: `create_directory(), create_file()`, and we did this so\n",
        "that we can actually do real things instead of just always creating the same folders over and over.\n",
        "\n",
        "Now, how can we actually put it all together so that given a task, a model can:\n",
        "\n",
        "- Plan the task\n",
        "- Execute actions to complete the task\n",
        "- Know when to call a function\n",
        "\n",
        "????\n",
        "\n",
        "This is actually an interesting problem, let's understand why is that the case by trying to hack our way into putting all of these together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rkghrDrdbTd",
        "outputId": "3671f4a1-81c1-4156-cb37-35fb1edbb7f5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "create_directory('lucas-the-agent-master')"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " class ModelWithTools:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "\n",
        "\n",
        "model = ModelWithTools(\"gpt-3.5-turbo-16k\")\n",
        "task_description = \"Create a folder called 'lucas-the-agent-master'. Inside that folder, create a file called 'the-10-master-rules.md\"\n",
        "output = model.get_response(f\"\"\"Given this task: {task_description}, \\n\n",
        "                            Consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
        "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
        "    \"\"\")\n",
        "\n",
        "Markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2EkHksJdbTd"
      },
      "source": [
        "Hey! Look at that the output is that function! Now, all we need is to direct this output to be executed somehow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXlKVujRdbTd"
      },
      "outputs": [],
      "source": [
        "exec(\"model.\" + output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4JArAd4dbTe",
        "outputId": "92c32c5a-7b06-4ae8-be6f-bd43c8d6ec16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[36mlucas-the-agent-master/\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls -d */ | grep lucas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-aEG07mdbTe"
      },
      "source": [
        "Yessss! We did it! All we had to do is to use the Python builtin method `exec` connected with the function call we got from the model's response! To avoid having to connect an outside function let's add some smart functionalities to our class to bind these capabilities all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcBK62mddbTe",
        "outputId": "1caf0fcb-e50d-474a-c7f5-686ef767f534"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "model.create_directory('lucas-the-unoriginal-joker')"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class ModelWithTools:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    def execute_function_call(self, function_call_string: str):\n",
        "        exec(function_call_string)\n",
        "\n",
        "\n",
        "model = ModelWithTools(\"gpt-3.5-turbo-16k\")\n",
        "task_description = \"Create a folder called 'lucas-the-unoriginal-joker'.\"\n",
        "output = model.get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
        "    For example:\n",
        "\n",
        "    task: 'create a folder named lucas-the-agent-master'\n",
        "    output: model.create_directory('lucas-the-agent-master')\n",
        "\n",
        "    task: 'create a file named the-10-master-rules.md'\n",
        "    output: model.create_file('the-10-master-rules.md')\n",
        "\n",
        "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
        "    task: {task_description}\n",
        "    output:\\n\n",
        "    \"\"\")\n",
        "\n",
        "Markdown(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1GoiY2ydbTe"
      },
      "source": [
        "Awesome! Now all we have to do is feed this to the model's `execute_function_call()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwS84N_6dbTe"
      },
      "outputs": [],
      "source": [
        "model.execute_function_call(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7voenh-dbTf",
        "outputId": "cba1ed07-5ec0-49bb-eef1-822364b46e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb\n",
            "building-agents.ipynb\n",
            "\u001b[1m\u001b[36mlucas-the-agent-master\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-unoriginal-joker\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mtest\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n",
            "test.txt\n"
          ]
        }
      ],
      "source": [
        "model.list_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2BOiHlkdbTf"
      },
      "source": [
        "And there we have it! We connected our model to the tools!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv1FAFN4dbTf"
      },
      "source": [
        "This is great, but what if we wanted to perform multiple actions?\n",
        "\n",
        "How about changing our prompt so that our output is a python list of function calls and then just looping over those lists and exeucting them one by one?\n",
        "\n",
        "Let's try that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOzqUnegdbTf",
        "outputId": "b9416ab9-d5a3-42f5-e6a0-49afcf74866f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "[model.create_directory('lucas-the-very-unoriginal-joker'), model.create_file('lucas-the-very-unoriginal-joker/the-10-unoriginal-rules-of-comedy.md')]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ModelWithTools(\"gpt-3.5-turbo-16k\")\n",
        "task_description = \"Create a folder called 'lucas-the-very-unoriginal-joker'. Inside that folder create a file called 'the-10-unoriginal-rules-of-comedy.md'.\"\n",
        "output = model.get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        subprocess.run([\"ls\"])\n",
        "    .\n",
        "    Your output should be the a list of function calls to be executed to complete the task containing the necessary arguments.\n",
        "    For example:\n",
        "\n",
        "    task: 'create a folder named test-dir'\n",
        "    output_list: [model.create_directory('test-dir')]\n",
        "\n",
        "    task: 'create a file named file.txt'\n",
        "    output_list: [model.create_file('file.txt')]\n",
        "\n",
        "    task: 'Create a folder named lucas-dir and inside that folder create a file named lucas-file.txt'\n",
        "    output_list: [model.create_directory('lucas-dir'), model.create_file('lucas-dir/lucas-file.txt')]\n",
        "\n",
        "    The OUTPUT SHOULD ONLY BE A PYTHON LIST WITH THE FUNCTION CALLS INSIDE and NOTHING ELSE.\n",
        "    task: {task_description}\n",
        "    output_list:\\n\n",
        "    \"\"\")\n",
        "\n",
        "Markdown(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COT0h8J-dbTf"
      },
      "outputs": [],
      "source": [
        "model.execute_function_call(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfYtnzcldbTg",
        "outputId": "958540d5-e092-41cd-b4db-a8733a476642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the-10-unoriginal-rules-of-comedy.md\n"
          ]
        }
      ],
      "source": [
        "!ls lucas-the-very-unoriginal-joker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGVJmveRdbTg"
      },
      "source": [
        "Yaaay we did it folks! All we had to do is to execute the output function calls using the `.execute_function_call()` method we created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL0tgQ-2dbTg"
      },
      "source": [
        "At this point we can start identifying a lot of issues with this approach despite our early sucess:\n",
        "\n",
        "- Uncertainty of model's outputs can affect our ability to reliably call the functions\n",
        "- We need more structured ways to prepare the inputs of the function calls\n",
        "- We need better ways to put everything together (just feeding the entire functions like this makes it a very clunky and non-scalable framework for more complex cases)\n",
        "\n",
        "There are many more issues but starting with these, we can now look at frameworks and see how they fix these issues and with that in mind understand what is behind their implementations!\n",
        "\n",
        "I personally think this is a much better way to understand what is going on behind agents in practice rather than just use the more higher level frameworks right of the bat!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5kMbi_CdbTg"
      },
      "source": [
        "# OpenAI Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIif8F5HdbTg"
      },
      "source": [
        "Ok, let's first understand how [OpenAI](https://openai.com/) the company behind ChatGPT, allows for these function call implementations in its API.\n",
        "\n",
        "OpenAI implemented a [function calling API](https://platform.openai.com/docs/guides/function-calling) which is a standard way to connect their models to outside tools like in the very simple example we did above.\n",
        "\n",
        "According to their [official documentation](https://platform.openai.com/docs/guides/function-calling#:~:text=The%20basic%20sequence,to%20the%20user.) the sequence of steps for function calling is as follows:\n",
        "1. Call the model with the user query and a set of functions defined in the functions parameter.\n",
        "2. The model can choose to call one or more functions; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may hallucinate parameters).\n",
        "3. Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
        "4. Call the model again by appending the function response as a new message, and let the model summarize the results back to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1_DYX4-dbTh"
      },
      "source": [
        "Below is an example taken from their official documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWx5ijwtdbTh",
        "outputId": "0fa14f5f-bf18-46c4-ed93-7933ff1e2320"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8ZPbnPT2X4zd4iypr9e4zK5BzGAvP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Currently in San Francisco, the temperature is 72°C. In Tokyo, the temperature is 10°C, and in Paris, the temperature is 22°C.', role='assistant', function_call=None, tool_calls=None))], created=1703450611, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_772e8125bb', usage=CompletionUsage(completion_tokens=33, prompt_tokens=175, total_tokens=208))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
        "    elif \"paris\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to the model\n",
        "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
        "    tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "    # Step 2: check if the model wanted to call a function\n",
        "    if tool_calls:\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        # Step 4: send the info for each function call and function response to the model\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(\n",
        "                location=function_args.get(\"location\"),\n",
        "                unit=function_args.get(\"unit\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )  # extend conversation with function response\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from the model where it can see the function response\n",
        "        return second_response\n",
        "output = run_conversation()\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmWc6YzFdbTh",
        "outputId": "790efb7f-914f-4c7f-8321-0f724078c87b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Currently in San Francisco, the temperature is 72°C. In Tokyo, the temperature is 10°C, and in Paris, the temperature is 22°C.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UVAREYQdbTi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def create_directory(directory_name):\n",
        "    \"\"\"Function that creates a directory given a directory name.\"\"\"\"\"\n",
        "    subprocess.run([\"mkdir\", directory_name])\n",
        "    return json.dumps({\"directory_name\": directory_name})\n",
        "\n",
        "\n",
        "tool_create_directory = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"create_directory\",\n",
        "        \"description\": \"Create a directory given a directory name.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"directory_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The name of the directory to create.\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"directory_name\"],\n",
        "        },\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttw7b3lPdbTi",
        "outputId": "65335c94-66a9-48ca-93cf-a5e7efb04741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8ZPgt072V0QZclotvAFGrSgVwEY2T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The folder called 'lucas-the-super-unoriginal-joker' has been created.\", role='assistant', function_call=None, tool_calls=None))], created=1703450927, model='gpt-3.5-turbo-16k-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=69, total_tokens=87))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def run_terminal_task():\n",
        "    messages = [{\"role\": \"user\", \"content\": \"Create a folder called 'lucas-the-super-unoriginal-joker'.\"}]\n",
        "    tools = [tool_create_directory]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "    # Step 2: check if the model wanted to call a function\n",
        "\n",
        "    if tool_calls:\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"create_directory\": create_directory,\n",
        "        }\n",
        "        messages.append(response_message)\n",
        "        # Step 4: send the info for each function call and function response to the model\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(\n",
        "                directory_name=function_args.get(\"directory_name\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "        )\n",
        "        return second_response\n",
        "\n",
        "output = run_terminal_task()\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMLQ6DxldbTi",
        "outputId": "153079e9-5cb8-41c0-cbb8-15f58531bf79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The folder called 'lucas-the-super-unoriginal-joker' has been created.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN4TYcPndbTi",
        "outputId": "401b6e89-32d5-43b2-b7da-0916b4952ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agents_script.ipynb              \u001b[1m\u001b[36mlucas-the-very-unoriginal-joker\u001b[m\u001b[m\n",
            "building-agents.ipynb            \u001b[1m\u001b[36mtest\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-agent-master\u001b[m\u001b[m           \u001b[1m\u001b[36mtest-dir\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-super-unoriginal-joker\u001b[m\u001b[m test.txt\n",
            "\u001b[1m\u001b[36mlucas-the-unoriginal-joker\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvj1lkMvdbTi"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiyZ9w82dbTi",
        "outputId": "5b0d936c-8561-4755-d403-fee8255dbc6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `create_directory` with `{'directory_name': 'lucas-the-random-joker2'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{\"directory_name\": \"lucas-the-random-joker2\"}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `create_file` with `{'file_path': 'lucas-the-random-joker2/the-10-random-rules-of-comedy.md'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m{\"file_path\": \"lucas-the-random-joker2/the-10-random-rules-of-comedy.md\"}\u001b[0m\u001b[32;1m\u001b[1;3mI have created a folder called 'lucas-the-random-joker2' and a file inside this folder called 'the-10-random-rules-of-comedy.md'.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Create a folder called 'lucas-the-random-joker2' and create a file inside this folder called 'the-10-random-rules-of-comedy.md'\",\n",
              " 'output': \"I have created a folder called 'lucas-the-random-joker2' and a file inside this folder called 'the-10-random-rules-of-comedy.md'.\"}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
        "\n",
        "@tool\n",
        "def create_directory(directory_name):\n",
        "    \"\"\"Function that creates a directory given a directory name.\"\"\"\"\"\n",
        "    subprocess.run([\"mkdir\", directory_name])\n",
        "    return json.dumps({\"directory_name\": directory_name})\n",
        "\n",
        "@tool\n",
        "def create_file(file_path):\n",
        "    \"\"\"Function that creates a file given a file path.\"\"\"\"\"\n",
        "    subprocess.run([\"touch\", file_path])\n",
        "    return json.dumps({\"file_path\": file_path})\n",
        "\n",
        "@tool\n",
        "def some_other_function():\n",
        "    \"\"\"Function that does something else.\"\"\"\"\"\n",
        "    return json.dumps({\"some\": \"response\"})\n",
        "\n",
        "tools = [create_directory, create_file]\n",
        "\n",
        "llm_chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "[\n",
        "    (\"system\",\"You are very powerful assistant that helps\\\n",
        "                users perform tasks in the terminal.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "llm_with_tools = llm_chat.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
        "\n",
        "agent = (\n",
        "{\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
        "        x[\"intermediate_steps\"]\n",
        "    ),\n",
        "}\n",
        "| prompt\n",
        "| llm_with_tools\n",
        "| OpenAIFunctionsAgentOutputParser())\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "action_input = \"Create a folder called 'lucas-the-random-joker2' and create a file inside this folder called 'the-10-random-rules-of-comedy.md'\"\n",
        "\n",
        "agent_executor.invoke({\"input\": action_input})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZYEAAHkdbTn",
        "outputId": "7aef9d36-7ab2-4055-9320-dff769409efc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nvz5ri-dbTo",
        "outputId": "0ed18d70-ab98-4bbd-e0c2-4d4a2e47033b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[36mlucas-the-agent-master/\u001b[m\u001b[m           \u001b[1m\u001b[36mlucas-the-unoriginal-joker/\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-random-joker/\u001b[m\u001b[m           \u001b[1m\u001b[36mlucas-the-very-unoriginal-joker/\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-random-joker2/\u001b[m\u001b[m          \u001b[1m\u001b[36mtest-dir/\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mlucas-the-super-unoriginal-joker/\u001b[m\u001b[m \u001b[1m\u001b[36mtest/\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls -d */"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBS2wH7IdbTo",
        "outputId": "b1bf245d-2c74-4865-dcda-6c1e0db76af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the-10-random-rules-of-comedy.md\n"
          ]
        }
      ],
      "source": [
        "!ls lucas-the-random-joker2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduahV_8dbTo"
      },
      "source": [
        "# References\n",
        "\n",
        "- [HuggingGPT](https://github.com/microsoft/JARVIS)\n",
        "- [Gen Agents](https://arxiv.org/pdf/2304.03442.pdf)\n",
        "- [WebGPT](https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22)\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
        "- [OpenAI](https://openai.com/)\n",
        "- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n",
        "- [GPT-Engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n",
        "- [BabyAGI](https://github.com/yoheinakajima/babyagi)\n",
        "- [Karpathy on Agents](https://www.youtube.com/watch?v=fqVLjtvWgq8)\n",
        "- [ReACT Paper](https://arxiv.org/abs/2210.03629)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oreilly-agents",
      "language": "python",
      "name": "oreilly-agents"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}